{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":94690,"databundleVersionId":11245741,"sourceType":"competition"}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"[Пример 1](https://www.kaggle.com/code/cornverburg/u-net-lung-segmentation-0-98-iou-99-5-accuracy)\n\n[Учебный материал по сегментации и детекции](https://github.com/EPC-MSU/EduNet-lectures/blob/dev-2.3/out/L11_Segmentation_Detection.ipynb)","metadata":{}},{"cell_type":"markdown","source":"### Введение\nВ блокноте демонстрируется подход на основе глубокого обучения к семантической сегментации рентгеновских изображений легких. Цель состоит в том, чтобы классифицировать пиксели рентгеновских изображений на два класса: легкие и фон. Будем использовать архитектуру U-Net с dropout-регуляризацией.\n\n### Набор данных\nНабор данных состоит из рентгеновских изображений и соответствующих масок. Изображения и маски загружаются с использованием пользовательского класса LungDataset.\n\n### Архитектура модели\nМодель семантической сегментации построена с использованием архитектуры U-Net, сочетающей возможности понижающей дискретизации и сверточных операций. В качестве функции потерь применена Dice Loss, используется оптимизатор Adam. Классическая архитектура U-Net показана на рисунке.\n![](https://camo.githubusercontent.com/6b548ee09b97874014d72903c891360beb0989e74b4585249436421558faa89d/68747470733a2f2f692e696d6775722e636f6d2f6a6544567071462e706e67)\n\n### Обучение и валидация\nБлокнот включает функции для обучения модели, визуализации примеров и вычисления метрик. Для предотвращения переобучения используется ранняя остановка (early stopping).\n\n### Результаты\nЗагружается обученная модель, и визуализируются примеры из валидационного набора. Для оценки производительности модели вычисляются такие метрики, как accuracy, IoU, F1 score, precision, recall.","metadata":{}},{"cell_type":"markdown","source":"### Импорт необходимых библиотек","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader, Subset, random_split\nfrom torchvision import transforms\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\n\ntry:\n    import segmentation_models_pytorch as smp\nexcept ImportError:\n    !pip install segmentation-models-pytorch -q > /dev/null\n    import segmentation_models_pytorch as smp","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-21T17:45:19.219882Z","iopub.execute_input":"2025-03-21T17:45:19.220302Z","iopub.status.idle":"2025-03-21T17:45:38.211545Z","shell.execute_reply.started":"2025-03-21T17:45:19.220264Z","shell.execute_reply":"2025-03-21T17:45:38.210554Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Наборы данных, загрузка и предварительная обработка данных\nОпределим пользовательский dataset-класс LungDataset для загрузки и предварительной обработки рентгеновских изображений легких и соответствующих им масок. Набор данных разделен на обучающую и валидационную выборки. Также определим преобразования (transform) для изменения размера и нормализации изображений и масок.\n\nДополнительно определим класс TestDataset для того, чтобы загружать изображения из тестового набора (здесь возвращаются только изображения без масок).","metadata":{}},{"cell_type":"code","source":"class LungDataset(Dataset):\n    def __init__(self, data_paths, transform=None, augmentations=None):\n        self.data_paths = sorted(data_paths)\n        self.transform = transform\n        self.augmentations = augmentations\n        \n        self.images = []\n        self.masks = []\n        \n        # Loop over data paths\n        for data_path in self.data_paths:\n            image_path = os.path.join(data_path, \"images\")\n            mask_path = os.path.join(data_path, \"masks\")\n            \n            # Load images and masks\n            images = [os.path.join(image_path, f) for f in os.listdir(image_path) if f.endswith(\".png\")]\n            masks = [os.path.join(mask_path, f) for f in os.listdir(mask_path) if f.endswith(\".png\")]\n\n            self.images.extend(sorted(images))\n            self.masks.extend(sorted(masks))\n            \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, idx):\n        image_path = self.images[idx]\n        mask_path = self.masks[idx]\n\n        # Open images and masks\n        image = Image.open(image_path).convert(\"L\")\n        mask = Image.open(mask_path).convert(\"L\")  # Convert to grayscale mask\n        \n        if self.transform:\n            image, mask = self.transform(image, mask)\n            \n        if self.augmentations:\n            image, mask = self.augmentations(image, mask)\n        \n        return image, mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T17:45:56.340761Z","iopub.execute_input":"2025-03-21T17:45:56.341067Z","iopub.status.idle":"2025-03-21T17:45:56.348399Z","shell.execute_reply.started":"2025-03-21T17:45:56.341045Z","shell.execute_reply":"2025-03-21T17:45:56.347284Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TestDataset(Dataset):\n    def __init__(self, data_paths, transform=None, augmentations=None):\n        self.data_paths = sorted(data_paths)\n        self.transform = transform\n        self.augmentations = augmentations\n        \n        self.images = []\n        \n        # Loop over data paths\n        for data_path in self.data_paths:\n            image_path = os.path.join(data_path, \"images\")\n            \n            # Load images\n            images = [os.path.join(image_path, f) for f in os.listdir(image_path) if f.endswith(\".png\")]\n\n            self.images.extend(sorted(images))\n            \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, idx):\n        image_path = self.images[idx]\n\n        # Open images\n        image = Image.open(image_path).convert(\"L\")\n        \n        if self.transform:\n            image = self.transform(image)\n            \n        if self.augmentations:\n            image = self.augmentations(image)\n        \n        return image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T17:45:58.409361Z","iopub.execute_input":"2025-03-21T17:45:58.409643Z","iopub.status.idle":"2025-03-21T17:45:58.415542Z","shell.execute_reply.started":"2025-03-21T17:45:58.40962Z","shell.execute_reply":"2025-03-21T17:45:58.414484Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Определим преобразования для изображений и масок","metadata":{}},{"cell_type":"code","source":"def transform(image, mask):\n    image_transform = transforms.Compose([\n        transforms.Resize(size=PATCH_SIZE, antialias=True),\n        transforms.ToTensor()\n    ])\n    \n    mask_transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Resize(size=PATCH_SIZE, antialias=False)\n    ])\n    \n    return image_transform(image), mask_transform(mask).type(torch.int)\n\n\ndef test_transform(image):\n    image_transform = transforms.Compose([\n        transforms.Resize(size=PATCH_SIZE, antialias=True),\n        transforms.ToTensor()\n    ])\n    \n    return image_transform(image)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T17:46:01.386863Z","iopub.execute_input":"2025-03-21T17:46:01.387209Z","iopub.status.idle":"2025-03-21T17:46:01.392277Z","shell.execute_reply.started":"2025-03-21T17:46:01.387182Z","shell.execute_reply":"2025-03-21T17:46:01.39125Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Создадим объекты классов Dataset и DataLoader","metadata":{}},{"cell_type":"code","source":"PATCH_SIZE = (256, 256)\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nBATCH_SIZE = 16\n\n\ndata_path = [\"/kaggle/input/radiograph-segmentation-2025/Dataset/COVID\",\n             \"/kaggle/input/radiograph-segmentation-2025/Dataset/Lung_Opacity\",\n             \"/kaggle/input/radiograph-segmentation-2025/Dataset/Normal\",\n             \"/kaggle/input/radiograph-segmentation-2025/Dataset/Viral Pneumonia\"]\n\ndataset = LungDataset(data_path, transform=transform, augmentations=None)\n\n\ntest_data_path = [\"/kaggle/input/radiograph-segmentation-2025/Test/\",]\ntest_dataset = TestDataset(test_data_path, transform=test_transform, augmentations=None)\n\n\n\n# Define the sizes for each split\ndataset_size = len(dataset)\nval_size  = int(0.20 * dataset_size)\ntrain_size = dataset_size - val_size\n\n\n# Use random_split to create train and val datasets\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n\n\n# train_dataset.augmentations = train_augmentations\n\n# Create DataLoader instances for each set\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T17:46:01.793757Z","iopub.execute_input":"2025-03-21T17:46:01.794032Z","iopub.status.idle":"2025-03-21T17:46:02.358597Z","shell.execute_reply.started":"2025-03-21T17:46:01.794012Z","shell.execute_reply":"2025-03-21T17:46:02.357585Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Отобразим некоторые примеры","metadata":{}},{"cell_type":"code","source":"# Function to visualize images and masks\ndef visualize_samples(dataset, num_samples=5):\n    # Visualize the images and masks\n    fig, axes = plt.subplots(num_samples, 2, figsize=(8, 2 * num_samples))\n    for i in range(num_samples):\n        image, mask = dataset[np.random.randint(len(dataset))]\n        \n        # Display images\n        axes[i, 0].imshow(image.cpu().permute(1,2,0))\n        axes[i, 0].set_title(f'Sample {i + 1} - Image')\n        axes[i, 0].axis('off')\n\n        # Display masks\n        axes[i, 1].imshow(mask.cpu().squeeze(), cmap='gray')\n        axes[i, 1].set_title(f'Sample {i + 1} - Mask')\n        axes[i, 1].axis('off')\n\n    plt.tight_layout()\n    plt.show()\n\n# Visualize samples from the train dataset\nvisualize_samples(train_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T17:46:15.895354Z","iopub.execute_input":"2025-03-21T17:46:15.895652Z","iopub.status.idle":"2025-03-21T17:46:16.778219Z","shell.execute_reply.started":"2025-03-21T17:46:15.89563Z","shell.execute_reply":"2025-03-21T17:46:16.777185Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Определение модели\nДалее определим архитектуру модели U-Net для семантической сегментации. Модель построена на архитектуре encoder-decoder с применением skip connections, что облегчает точную локализацию признаков. Для уменьшения переобучения добавлены dropout-слои.","metadata":{}},{"cell_type":"markdown","source":"### Определим UNet блоки (Up, DoubleConv, OutConv)","metadata":{}},{"cell_type":"code","source":"class DoubleConv(nn.Module):\n    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n\n    def __init__(self, in_channels, out_channels, mid_channels=None, dropout_rate=0.1):\n        super().__init__()\n        if not mid_channels:\n            mid_channels = out_channels\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(mid_channels),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout_rate),\n            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout_rate)\n        )\n\n    def forward(self, x):\n        return self.double_conv(x)\n\n\nclass Down(nn.Module):\n    \"\"\"Downscaling with maxpool then double conv\"\"\"\n\n    def __init__(self, in_channels, out_channels, dropout_rate=0.1):\n        super().__init__()\n        self.maxpool_conv = nn.Sequential(\n            nn.MaxPool2d(2),\n            DoubleConv(in_channels, out_channels, dropout_rate=dropout_rate)\n        )\n\n    def forward(self, x):\n        return self.maxpool_conv(x)\n\n\nclass Up(nn.Module):\n    \"\"\"Upscaling then double conv\"\"\"\n\n    def __init__(self, in_channels, out_channels, bilinear=True, dropout_rate=0.1):\n        super().__init__()\n\n        # if bilinear, use the normal convolutions to reduce the number of channels\n        if bilinear:\n            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2, dropout_rate=dropout_rate)\n        else:\n            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n            self.conv = DoubleConv(in_channels, out_channels, dropout_rate=dropout_rate)\n\n    def forward(self, x1, x2):\n        x1 = self.up(x1)\n        # input is CHW\n        diffY = x2.size()[2] - x1.size()[2]\n        diffX = x2.size()[3] - x1.size()[3]\n\n        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n                        diffY // 2, diffY - diffY // 2])\n        # if you have padding issues, see\n        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n        x = torch.cat([x2, x1], dim=1)\n        return self.conv(x)\n\n\nclass OutConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(OutConv, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n\n    def forward(self, x):\n        return self.conv(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T17:46:24.416615Z","iopub.execute_input":"2025-03-21T17:46:24.416922Z","iopub.status.idle":"2025-03-21T17:46:24.428543Z","shell.execute_reply.started":"2025-03-21T17:46:24.416898Z","shell.execute_reply":"2025-03-21T17:46:24.427747Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Определим U-Net модель","metadata":{}},{"cell_type":"code","source":"class UNet(nn.Module):\n    def __init__(self, n_channels, n_classes, bilinear=False, dropout_rate=0.1):\n        super(UNet, self).__init__()\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n        self.bilinear = bilinear\n\n        self.inc = (DoubleConv(n_channels, 64, dropout_rate=dropout_rate))\n        self.down1 = (Down(64, 128, dropout_rate=dropout_rate))\n        self.down2 = (Down(128, 256, dropout_rate=dropout_rate))\n        self.down3 = (Down(256, 512, dropout_rate=dropout_rate))\n        factor = 2 if bilinear else 1\n        self.down4 = (Down(512, 1024 // factor, dropout_rate=dropout_rate))\n        self.up1 = (Up(1024, 512 // factor, bilinear, dropout_rate=dropout_rate))\n        self.up2 = (Up(512, 256 // factor, bilinear, dropout_rate=dropout_rate))\n        self.up3 = (Up(256, 128 // factor, bilinear, dropout_rate=dropout_rate))\n        self.up4 = (Up(128, 64, bilinear, dropout_rate=dropout_rate))\n        self.outc = (OutConv(64, n_classes))\n\n    def forward(self, x):\n        x1 = self.inc(x)\n        x2 = self.down1(x1)\n        x3 = self.down2(x2)\n        x4 = self.down3(x3)\n        x5 = self.down4(x4)\n        x = self.up1(x5, x4)\n        x = self.up2(x, x3)\n        x = self.up3(x, x2)\n        x = self.up4(x, x1)\n        logits = self.outc(x)\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T17:46:27.706507Z","iopub.execute_input":"2025-03-21T17:46:27.706815Z","iopub.status.idle":"2025-03-21T17:46:27.71454Z","shell.execute_reply.started":"2025-03-21T17:46:27.706792Z","shell.execute_reply":"2025-03-21T17:46:27.71356Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Создадим объект модели UNet","metadata":{}},{"cell_type":"code","source":"unet = UNet(1, 2, bilinear=False, dropout_rate=0.1).to(DEVICE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T17:46:30.337663Z","iopub.execute_input":"2025-03-21T17:46:30.337968Z","iopub.status.idle":"2025-03-21T17:46:30.862633Z","shell.execute_reply.started":"2025-03-21T17:46:30.337946Z","shell.execute_reply":"2025-03-21T17:46:30.861946Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Настройка обучения\nНастроим процесс обучения, определяя функцию потерь, оптимизатор и планировщик скорости обучения (learning rate scheduler). Применим функцию потерь Dice для многоклассовой сегментации и оптимизатор Adam. Кроме того, настроим learning rate scheduler так, чтобы корректировать скорость обучения learning rate в зависимости от потери на валидационной выборке. Также определим критерий ранней остановки (early stopping) для предотвращения переобучения.","metadata":{}},{"cell_type":"code","source":"import torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\n# Define constants\nLEARNING_RATE = 0.0003\nLR_FACTOR = 0.5\nLR_PATIENCE = 2\nEARLY_STOP_PATIENCE = 4\nNUM_EPOCHS = 4\n\n# Define the loss function, optimizer, and learning rate scheduler\ncriterion = smp.losses.DiceLoss('multiclass')\noptimizer = optim.Adam(unet.parameters(), lr=LEARNING_RATE)\nscheduler = ReduceLROnPlateau(optimizer, mode='min', factor=LR_FACTOR, patience=LR_PATIENCE, verbose=False)\n\n# Initialize early stopping parameters\nearly_stop_counter = 0\nbest_val_loss = float('inf')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T17:46:38.611147Z","iopub.execute_input":"2025-03-21T17:46:38.6115Z","iopub.status.idle":"2025-03-21T17:46:38.619844Z","shell.execute_reply.started":"2025-03-21T17:46:38.61147Z","shell.execute_reply":"2025-03-21T17:46:38.618909Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Обучим модель","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nfrom torch.nn.parallel import DataParallel\n\ndef train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=20, early_stop_patience=4):\n    best_val_loss = float('inf')\n    early_stop_counter = 0\n\n    train_losses = []\n    val_losses = []\n\n    model = DataParallel(model)\n    \n    for epoch in range(1, num_epochs + 1):\n        model.train()\n        train_loss = 0.0\n\n        for batch_idx, (images, masks) in tqdm(enumerate(train_loader), total=len(train_loader)):\n            images, masks = images.to(DEVICE), masks.to(DEVICE, dtype=torch.long)\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, masks)\n            \n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n\n        avg_train_loss = train_loss / len(train_loader)\n        train_losses.append(avg_train_loss)\n\n        # Validation phase\n        model.eval()\n        val_loss = 0.0\n\n        with torch.no_grad():\n            for images, masks in val_loader:\n                images, masks = images.to(DEVICE), masks.to(DEVICE, dtype=torch.long)\n                outputs = model(images)\n                loss = criterion(outputs, masks)\n                val_loss += loss.item()\n\n        avg_val_loss = val_loss / len(val_loader)\n        val_losses.append(avg_val_loss)\n\n        # Update learning rate scheduler\n        scheduler.step(avg_val_loss)\n\n        # Print and check for early stopping\n        print(f'Epoch [{epoch}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')\n\n        if avg_val_loss < best_val_loss:\n            torch.save(model.state_dict(), 'best_model.pth')  # Save the best model\n            best_val_loss = avg_val_loss\n            early_stop_counter = 0\n        else:\n            early_stop_counter += 1\n\n        if early_stop_counter >= early_stop_patience:\n            print(f'Early stopping after {early_stop_patience} epochs without improvement.')\n            break\n\n    return train_losses, val_losses\n\n# Now, call the function with your specific parameters\ntrain_model(unet, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=NUM_EPOCHS, early_stop_patience=EARLY_STOP_PATIENCE)\n\n# Load the best model after training\nunet = nn.DataParallel(unet) # this is necessary to get matching key dicts\nunet.load_state_dict(torch.load('best_model.pth'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T17:46:43.246966Z","iopub.execute_input":"2025-03-21T17:46:43.247387Z","iopub.status.idle":"2025-03-21T18:30:36.025578Z","shell.execute_reply.started":"2025-03-21T17:46:43.247351Z","shell.execute_reply":"2025-03-21T18:30:36.024427Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Отобразим примеры из валидационного набора","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport torchvision.transforms.functional as F\n\n# Function to plot examples with predicted and true masks\ndef plot_examples(model, dataset, num_examples=5):\n    model.eval()\n    \n    for i in range(num_examples):\n        image, mask = dataset[i]\n        \n        with torch.no_grad():\n            output = model(image.unsqueeze(0).to(DEVICE)).cpu()\n        \n        pred_mask = torch.argmax(output, dim=1)\n        \n        # Plot the images and masks\n        plt.figure(figsize=(12, 4))\n        \n        plt.subplot(1, 3, 1)\n        plt.imshow(image.permute(1,2,0), cmap='gray')\n        plt.axis(\"off\")\n        plt.title('Image')\n        \n        plt.subplot(1, 3, 2)\n        plt.imshow(mask.permute(1,2,0), cmap='gray')\n        plt.axis(\"off\")\n        plt.title('True Mask')\n        \n        plt.subplot(1, 3, 3)\n        plt.imshow(pred_mask.permute(1,2,0), cmap='gray')\n        plt.axis(\"off\")\n        plt.title('Predicted Mask')\n        \n        plt.tight_layout()\n        plt.show()\n\n# Plot examples from the test dataloader\nplot_examples(unet, val_dataset, num_examples=10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T18:30:36.02703Z","iopub.execute_input":"2025-03-21T18:30:36.027327Z","iopub.status.idle":"2025-03-21T18:30:39.645682Z","shell.execute_reply.started":"2025-03-21T18:30:36.0273Z","shell.execute_reply":"2025-03-21T18:30:39.644814Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Рассчитаем метрики на валидационном наборе","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, jaccard_score, f1_score, precision_score, recall_score\n\ndef compute_metrics(model, dataloader):\n    model.eval()\n    \n    all_true_masks = []\n    all_pred_masks = []\n    \n    confusion = np.zeros((2,2))\n    for images, masks in tqdm(dataloader, total=len(dataloader)):\n        images, masks = images.to(DEVICE), masks.to(DEVICE)\n        \n        with torch.no_grad():\n            outputs = model(images)\n        \n        # Convert probability maps to binary masks using a threshold\n        pred_masks = torch.argmax(outputs, dim=1)\n        \n        true_masks_np = masks.cpu().detach().numpy().ravel()\n        pred_masks_np = pred_masks.cpu().detach().numpy().ravel()\n        \n        confusion += confusion_matrix(true_masks_np, pred_masks_np)\n    \n    # Calculate metrics\n    TN, FP, FN, TP = confusion.ravel()\n    accuracy = (TP + TN) / (TP + TN + FP + FN)\n    precision = TP / (TP + FP)\n    recall = TP / (TP + FN)\n    f1_score = 2 * (precision * recall) / (precision + recall)\n    jaccard_index = TP / (TP + FP + FN)\n\n    print(f\"  Accuracy : {accuracy:.4f}\")\n    print(f\"  IoU      : {jaccard_index:.4f}\")\n    print(f\"  F1 Score : {f1_score:.4f}\")\n    print(f\"  Precision: {precision:.4f}\")\n    print(f\"  Recall   : {recall:.4f}\")\n\n# Ensure DEVICE variable is defined\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Compute metrics for validation set\nprint(\"\\nMetrics for Validation Set:\")\ncompute_metrics(unet, val_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T18:30:56.223017Z","iopub.execute_input":"2025-03-21T18:30:56.223374Z","iopub.status.idle":"2025-03-21T18:32:59.946626Z","shell.execute_reply.started":"2025-03-21T18:30:56.223347Z","shell.execute_reply":"2025-03-21T18:32:59.945537Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Predict на тестовой выборке и последующее формирование файла submission","metadata":{}},{"cell_type":"code","source":"# Generate predictions\npred_masks = []\n\nunet.eval()\n    \nfor image in test_dataset:\n\n    with torch.no_grad():\n        output = unet(image.unsqueeze(0).to(DEVICE)).cpu()\n\n    pred_mask = torch.argmax(output, dim=1).cpu().detach().numpy()\n    pred_masks.append(pred_mask)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T18:32:59.947972Z","iopub.execute_input":"2025-03-21T18:32:59.948256Z","iopub.status.idle":"2025-03-21T18:33:08.795164Z","shell.execute_reply.started":"2025-03-21T18:32:59.948228Z","shell.execute_reply":"2025-03-21T18:33:08.794467Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(pred_masks)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T18:33:08.79651Z","iopub.execute_input":"2025-03-21T18:33:08.796739Z","iopub.status.idle":"2025-03-21T18:33:08.801929Z","shell.execute_reply.started":"2025-03-21T18:33:08.796719Z","shell.execute_reply":"2025-03-21T18:33:08.801109Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# RLE-кодировка / декодировка для submission\ndef encode_mask_to_rle(mask):\n    '''\n    mask: бинарная маска в виде numpy массива \n    1 - объект\n    0 - фон\n    Возвращает закодированную длину серии \n    '''\n    pixels = mask.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n\ndef decode_rle_to_mask(rle, height, width, viz = False):\n    '''\n    rle : длина серии в строковом формате (начальное значение, число элементов)\n    height : высота изображения маски \n    width : ширина изображения маски\n    Возвращает бинарную маску\n    '''\n    rle = np.array(rle.split(' ')).reshape(-1, 2)\n    mask = np.zeros((height * width, 1, 3))\n    if viz:\n        color = np.random.rand(3)\n    else:\n        color = [1, 1, 1]\n    for i in rle:\n        mask[int(i[0]):int(i[0]) + int(i[1]), :, :] = color\n        \n    return cv2.cvtColor(mask.reshape(height, width, 3).astype(np.uint8), cv2.COLOR_BGR2GRAY)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T18:33:08.803027Z","iopub.execute_input":"2025-03-21T18:33:08.803358Z","iopub.status.idle":"2025-03-21T18:33:08.818473Z","shell.execute_reply.started":"2025-03-21T18:33:08.803326Z","shell.execute_reply":"2025-03-21T18:33:08.817826Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# RLE-кодирование предсказаний\nrle_pred = []\nfor i in range(len(pred_masks)):\n    encoded = encode_mask_to_rle(pred_masks[i])\n    rle_pred.append(encoded)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T18:33:08.819226Z","iopub.execute_input":"2025-03-21T18:33:08.81943Z","iopub.status.idle":"2025-03-21T18:33:08.907072Z","shell.execute_reply.started":"2025-03-21T18:33:08.819412Z","shell.execute_reply":"2025-03-21T18:33:08.906504Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(rle_pred)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T18:33:08.90771Z","iopub.execute_input":"2025-03-21T18:33:08.907915Z","iopub.status.idle":"2025-03-21T18:33:08.912751Z","shell.execute_reply.started":"2025-03-21T18:33:08.907897Z","shell.execute_reply":"2025-03-21T18:33:08.911926Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Данные для submission\nimport pandas as pd\n\npred = pd.DataFrame(data = {'id':range(len(rle_pred)), \n                          'target':rle_pred})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T18:33:08.913564Z","iopub.execute_input":"2025-03-21T18:33:08.913854Z","iopub.status.idle":"2025-03-21T18:33:09.201887Z","shell.execute_reply.started":"2025-03-21T18:33:08.913825Z","shell.execute_reply":"2025-03-21T18:33:09.201266Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Вывод данных\npred","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T18:33:09.203487Z","iopub.execute_input":"2025-03-21T18:33:09.204164Z","iopub.status.idle":"2025-03-21T18:33:09.226375Z","shell.execute_reply.started":"2025-03-21T18:33:09.204142Z","shell.execute_reply":"2025-03-21T18:33:09.225728Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Создание csv-файла для submission\npred.to_csv('/kaggle/working/submission.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T18:33:09.227207Z","iopub.execute_input":"2025-03-21T18:33:09.227485Z","iopub.status.idle":"2025-03-21T18:33:09.247699Z","shell.execute_reply.started":"2025-03-21T18:33:09.227456Z","shell.execute_reply":"2025-03-21T18:33:09.247132Z"}},"outputs":[],"execution_count":null}]}