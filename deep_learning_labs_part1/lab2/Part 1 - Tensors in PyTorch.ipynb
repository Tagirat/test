{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введение в глубокое обучение с PyTorch\n",
    "\n",
    "В этом блокноте вы познакомитесь с [PyTorch](http://pytorch.org/), фреймворком для создания и обучения нейронных сетей. Массивы в PyTorch во многом ведут себя как массивы Numpy. Обобщением массивов являются тензоры. PyTorch работает с этими тензорами и упрощает их перенос на GPU, чтобы быстрее обучать нейронные сети. Он также предоставляет модуль, который автоматически вычисляет градиенты (для обратного распространения) и еще один модуль, предназначенный специально для построения нейронных сетей. В целом, PyTorch оказывается более совместимым с Python и стеком Numpy/Scipy по сравнению с TensorFlow и другими фреймворками.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нейронные сети\n",
    "\n",
    "Глубокое обучение основано на искусственных нейронных сетях, которые существуют в той или иной форме с конца 1950-х годов. Сети состоят из отдельных частей, подобных нейронам, которые обычно называются узлами или просто \"нейронами\". Каждый узел имеет некоторое количество взвешенных входов. Эти взвешенные входы суммируются (линейная комбинация), а затем передаются на активационную функцию, чтобы получить выход узла.\n",
    "\n",
    "<img src=\"assets/simple_neuron.png\" width=400px>\n",
    "\n",
    "Математически это выглядит следующим образом: \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y &= f(w_1 x_1 + w_2 x_2 + b) \\\\\n",
    "y &= f\\left(\\sum_i w_i x_i +b \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "С векторами это скалярное произведение двух векторов:\n",
    "\n",
    "$$\n",
    "h = \\begin{bmatrix}\n",
    "x_1 \\, x_2 \\cdots  x_n\n",
    "\\end{bmatrix}\n",
    "\\cdot \n",
    "\\begin{bmatrix}\n",
    "           w_1 \\\\\n",
    "           w_2 \\\\\n",
    "           \\vdots \\\\\n",
    "           w_n\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тензоры\n",
    "\n",
    "Вычисления в нейронных сетях являются просто множеством операций линейной алгебры над *тензорами*, обобщением матриц. Вектор — это 1-мерный тензор, матрица — это 2-мерный тензор, массив с тремя индексами — это 3-мерный тензор (например, цветные изображения RGB). Основная структура данных для нейронных сетей — это тензоры, и PyTorch (как и практически каждый другой фреймворк глубокого обучения) построен вокруг тензоров.\n",
    "\n",
    "<img src=\"assets/tensor_examples.svg\" width=600px>\n",
    "\n",
    "Попробуем применить PyTorch для построения простой нейронной сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сначала импортируем PyTorch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x):\n",
    "    \"\"\" Сигмоидная Функция активации\n",
    "    \n",
    "        Аргументы\n",
    "        ---------\n",
    "        x: torch.Tensor\n",
    "    \"\"\"\n",
    "    return 1/(1+torch.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Генерация данных\n",
    "torch.manual_seed(7) # Устанавливаем seed для воспроизводимости результатов\n",
    "\n",
    "# Признаки — это 5 случайных переменных из нормального распределения\n",
    "features = torch.randn((1, 5))\n",
    "# Истинные (true) веса для наших данных, снова случайные переменные из нормального распределения\n",
    "weights = torch.randn_like(features)\n",
    "# и истинное (true) смещение\n",
    "bias = torch.randn((1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выше сгенерированы данные, которые мы можем использовать, чтобы получить выход нашей простой сети. На данный момент все это просто случайные данные из нормального распределения. Пройдемся по каждой строке:\n",
    "\n",
    "`features = torch.randn((1, 5))` создает тензор с формой `(1, 5)`, одной строкой и пятью столбцами, который содержит значения, случайно распределенные в соответствии с нормальным распределением со средним значением ноль и стандартным отклонением один. \n",
    "\n",
    "`weights = torch.randn_like(features)` создает другой тензор с такой же формой, как и `features`, снова содержащий значения из нормального распределения.\n",
    "\n",
    "Наконец, `bias = torch.randn((1, 1))` создает одно значение из нормального распределения.\n",
    "\n",
    "Тензоры PyTorch можно складывать, умножать, вычитать и так далее, точно так же, как и массивы Numpy. В общем, вы будете использовать тензоры PyTorch практически так же, как вы бы использовали массивы Numpy. Однако они имеют несколько приятных преимуществ, таких как ускорение с помощью GPU, к которому мы перейдем позже. А пока используйте сгенерированные данные для вычисления выхода этой простой однослойной сети. \n",
    "> **Упражнение**: Вычислите выход сети с входными признаками `features`, весами `weights` и смещением `bias`. Подобно Numpy, PyTorch имеет функцию [`torch.sum()`](https://pytorch.org/docs/stable/torch.html#torch.sum), а также метод `.sum()` на тензорах для суммирования. Используйте функцию `activation`, определенную выше, в качестве функции активации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1595]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TODO\n",
    "activation(torch.matmul(features,weights.T) + bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вы можете произвести умножение и суммирование в одной операции, используя матричное умножение. В общем случае, вы предпочтительно использовать матричное умножение, так как оно более эффективно и ускоряется с помощью современных библиотек и высокопроизводительных вычислений на GPU.\n",
    "\n",
    "Попробуем сделать матричное умножение признаков и весов. Для этого мы можем использовать [`torch.mm()`](https://pytorch.org/docs/stable/torch.html#torch.mm) или [`torch.matmul()`](https://pytorch.org/docs/stable/torch.html#torch.matmul), которые несколько более сложные. Если мы попробуем сделать это с `features` и `weights` в их текущем виде, мы получим ошибку\n",
    "\n",
    "```python\n",
    ">> torch.mm(features, weights)\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "RuntimeError                              Traceback (most recent call last)\n",
    "<ipython-input-13-15d592eb5279> in <module>()\n",
    "----> 1 torch.mm(features, weights)\n",
    "\n",
    "RuntimeError: size mismatch, m1: [1 x 5], m2: [1 x 5] at /Users/soumith/minicondabuild3/conda-bld/pytorch_1524590658547/work/aten/src/TH/generic/THTensorMath.c:2033\n",
    "```\n",
    "\n",
    "При создании нейронных сетей в любом фреймворке вы будете довольно часто сталкиваться с этим типом ошибок. В данном случае наши тензоры не имеют правильных размерностей для выполнения матричного умножения. Вспомним, что для матричного умножения количество столбцов в первом тензоре должно быть равно количеству строк во втором тензоре. Оба `features` и `weights` имеют одну и ту же форму (shape) `(1, 5)`. Это означает, что нам нужно изменить форму `weights`, чтобы выполнить матричное умножение.\n",
    "\n",
    "**Примечание:** Чтобы увидеть форму тензора, называемого `tensor`, используйте `tensor.shape`. При построении нейронных сетей вы будете часто использовать этот метод.\n",
    "\n",
    "Есть несколько вариантов, как изменить размеры (shape) тензора: [`weights.reshape()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.reshape), [`weights.resize_()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.resize_), и [`weights.view()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view).\n",
    "\n",
    "* `weights.reshape(a, b)` вернет новый тензор с теми же данными (иногда), что и `weights`, размером `(a, b)`, а иногда клона, так как копирует данные в другую часть памяти.\n",
    "* `weights.resize_(a, b)` возвращает тот же тензор другого размера. Однако, если новый размер приводит к меньшему количеству элементов, чем оригинальный тензор, некоторые элементы будут удалены из тензора (но не из памяти). Если новая форма приводит к большему количеству элементов, чем оригинальный тензор, новые элементы будут неинициализированными в памяти. Подчеркивание в конце метода указывает на то, что этот метод выполняется **на месте**. Вот отличная тематическая беседа, чтобы [узнать больше о операциях на месте](https://discuss.pytorch.org/t/what-is-in-place-operation/16244) в PyTorch.\n",
    "* `weights.view(a, b)` вернет новый тензор с теми же данными, что и `weights`, размером `(a, b)`.\n",
    "\n",
    "Обычно используется `.view()`, но любой из трех методов подойдет для этой задачи. Итак, теперь мы можем изменить форму `weights`, чтобы иметь пять строк и один столбец, применив что-то вроде `weights.view(5, 1)`.\n",
    "\n",
    "> **Упражнение**: Вычислите выход нашей маленькой сети, используя матричное умножение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1595]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TODO\n",
    "activation(torch.matmul(features,weights.view(5,1)) + bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сложите их вместе\n",
    "\n",
    "Вы можете вычислить выход для одного нейрона. Реальная мощь этого алгоритма проявляется, когда вы начинаете объединять эти отдельные узлы в слои и наборы слоев, в сеть нейронов. Выход одного слоя нейронов становится входом для следующего слоя. Теперь нам нужно выразить веса в виде матрицы.\n",
    "\n",
    "<img src='assets/multilayer_diagram_weights.png' width=450px>\n",
    "\n",
    "Первый слой, показанный внизу, это входы, называемые **входным слоем**. Средний слой называется **скрытым слоем**, а последний слой (справа) — **выходным слоем**. Мы можем выразить эту сеть математически с помощью матриц и использовать матричное умножение, чтобы получить линейные комбинации для каждой узла в одной операции. Например, скрытый слой ($h_1$ и $h_2$ здесь) можно вычислить \n",
    "\n",
    "$$\n",
    "\\vec{h} = [h_1 \\, h_2] = \n",
    "\\begin{bmatrix}\n",
    "x_1 \\, x_2 \\cdots \\, x_n\n",
    "\\end{bmatrix}\n",
    "\\cdot \n",
    "\\begin{bmatrix}\n",
    "           w_{11} & w_{12} \\\\\n",
    "           w_{21} &w_{22} \\\\\n",
    "           \\vdots &\\vdots \\\\\n",
    "           w_{n1} &w_{n2}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Выход для этой маленькой сети найдем, рассматривая скрытый слой как входы для выходного узла. Выход сети выражается как\n",
    "\n",
    "$$\n",
    "y =  f_2 \\! \\left(\\, f_1 \\! \\left(\\vec{x} \\, \\mathbf{W_1}\\right) \\mathbf{W_2} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Генерация данных\n",
    "torch.manual_seed(7) # Устанавливаем seed для воспроизводимости результатов\n",
    "\n",
    "# Признаки — это 3 случайные переменные из нормального распределения\n",
    "features = torch.randn((1, 3))\n",
    "\n",
    "# Определим размер каждого слоя в нашей сети\n",
    "n_input = features.shape[1]     # Количество входных узлов, должно совпадать с количеством входных признаков\n",
    "n_hidden = 2                    # Количество скрытых узлов \n",
    "n_output = 1                    # Количество выходных узлов\n",
    "\n",
    "# Веса для входов к скрытому слою\n",
    "W1 = torch.randn(n_input, n_hidden)\n",
    "# Веса для скрытого слоя к выходному слою\n",
    "W2 = torch.randn(n_hidden, n_output)\n",
    "\n",
    "# и смещения для скрытого и выходного слоев\n",
    "B1 = torch.randn((1, n_hidden))\n",
    "B2 = torch.randn((1, n_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Упражнение:** Вычислите выход для этой многослойной сети, используя веса `W1` и `W2`, а также смещения `B1` и `B2`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3171]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TODO\n",
    "a1 = activation(features.matmul(W1) + B1)\n",
    "activation(a1.matmul(W2) + B2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если вы сделали это правильно, то должны увидеть выход `tensor([[ 0.3171]])`.\n",
    "\n",
    "Количество скрытых узлов — это параметр сети, часто называемый **гиперпараметром**, чтобы отличать его от параметров весов и смещения. Как вы увидите позже, когда мы будем обсуждать обучение нейронной сети, чем больше скрытых единиц имеет сеть и чем больше слоев, тем лучше она может учиться на данных и делать более точные предсказания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy в Torch и обратно\n",
    "\n",
    "PyTorch имеет удобные функции для преобразования между массивами Numpy и тензорами Torch. Чтобы создать тензор из массива Numpy, используйте `torch.from_numpy()`. Чтобы преобразовать тензор в массив Numpy, используйте метод `.numpy()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.93931607, 0.14659713, 0.67016391],\n",
       "       [0.43523031, 0.37527432, 0.63085754],\n",
       "       [0.58193968, 0.44397625, 0.48643968],\n",
       "       [0.95365355, 0.38472207, 0.69942869]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.random.rand(4,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9393, 0.1466, 0.6702],\n",
       "        [0.4352, 0.3753, 0.6309],\n",
       "        [0.5819, 0.4440, 0.4864],\n",
       "        [0.9537, 0.3847, 0.6994]], dtype=torch.float64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.from_numpy(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.93931607, 0.14659713, 0.67016391],\n",
       "       [0.43523031, 0.37527432, 0.63085754],\n",
       "       [0.58193968, 0.44397625, 0.48643968],\n",
       "       [0.95365355, 0.38472207, 0.69942869]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Память разделяется между массивом Numpy и тензором Torch, так что если вы измените значения на месте (in-place) одного объекта, другой также изменится."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.8786, 0.2932, 1.3403],\n",
       "        [0.8705, 0.7505, 1.2617],\n",
       "        [1.1639, 0.8880, 0.9729],\n",
       "        [1.9073, 0.7694, 1.3989]], dtype=torch.float64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Умножаем тензор PyTorch на 2, in-place\n",
    "b.mul_(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.87863214, 0.29319427, 1.34032783],\n",
       "       [0.87046062, 0.75054864, 1.26171507],\n",
       "       [1.16387936, 0.8879525 , 0.97287935],\n",
       "       [1.90730709, 0.76944414, 1.39885738]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Массив Numpy соответствует новому значению из Тензора\n",
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "YOLO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
