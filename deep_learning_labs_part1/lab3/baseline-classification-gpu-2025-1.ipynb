{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":93871,"databundleVersionId":11173287,"sourceType":"competition"}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"[Пример 1](https://www.kaggle.com/code/arunlukedsouza/covid-19-chest-x-ray-classification-with-resnet-18)\n\n[Пример 2](https://www.kaggle.com/code/arunrk7/covid-19-detection-pytorch-tutorial)","metadata":{}},{"cell_type":"markdown","source":"### Импорт библиотек","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport os\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport torchvision\nfrom torchvision.datasets import ImageFolder\n\nfrom torch.utils.data.dataloader import DataLoader\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import Subset\n\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport random\nfrom PIL import Image\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\n%matplotlib inline\ntorch.manual_seed(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T09:17:43.672102Z","iopub.execute_input":"2025-03-15T09:17:43.672399Z","iopub.status.idle":"2025-03-15T09:17:47.063415Z","shell.execute_reply.started":"2025-03-15T09:17:43.672376Z","shell.execute_reply":"2025-03-15T09:17:47.062504Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Проверка доступа к GPU","metadata":{}},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device=torch.device(\"cuda:0\")\n    print(\"Training on GPU...\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\"Training on CPU...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T09:17:47.064454Z","iopub.execute_input":"2025-03-15T09:17:47.064833Z","iopub.status.idle":"2025-03-15T09:17:47.092216Z","shell.execute_reply.started":"2025-03-15T09:17:47.064798Z","shell.execute_reply":"2025-03-15T09:17:47.091374Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Преобразования (Transforms)","metadata":{}},{"cell_type":"code","source":"# Creating a Transformation Object\ntrain_transform = torchvision.transforms.Compose([\n    # Converting images to the size that the model expects\n    torchvision.transforms.Resize(size=(224, 224)),\n    torchvision.transforms.RandomHorizontalFlip(), # A RandomHorizontalFlip to augment our data\n    torchvision.transforms.ToTensor(), # Converting to tensor\n    # Добавьте необходимую нормализацию,\n    # если будете применять предобученную модель, например, ResNet18\n    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                     std=[0.229, 0.224, 0.225]) # Normalizing the data to the data that the ResNet18 was trained on\n    \n])\n\n\nval_transform = torchvision.transforms.Compose([\n    # Converting images to the size that the model expects\n    torchvision.transforms.Resize(size=(224, 224)),\n    torchvision.transforms.ToTensor(), # Converting to tensor\n    # Добавьте необходимую нормализацию,\n    # если будете применять предобученную модель, например, ResNet18\n    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                     std=[0.229, 0.224, 0.225]) # Normalizing the data to the data that the ResNet18 was trained on\n    \n])\n\n\ntest_transform = torchvision.transforms.Compose([\n    # Converting images to the size that the model expects\n    torchvision.transforms.Resize(size=(224, 224)),\n    torchvision.transforms.ToTensor(), # Converting to tensor\n    # Добавьте необходимую нормализацию,\n    # если будете применять предобученную модель, например, ResNet18\n    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                     std=[0.229, 0.224, 0.225]) # Normalizing the data to the data that the ResNet18 was trained on\n    \n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T09:17:47.094013Z","iopub.execute_input":"2025-03-15T09:17:47.094298Z","iopub.status.idle":"2025-03-15T09:17:47.109954Z","shell.execute_reply.started":"2025-03-15T09:17:47.094278Z","shell.execute_reply":"2025-03-15T09:17:47.10917Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Создание объектов Datasets с разбиением выборки на обучающую и валидационную","metadata":{}},{"cell_type":"code","source":"train_val_path=\"/kaggle/input/radiograph-classification-2025/Dataset\"\n\ntrain_dataset = ImageFolder(train_val_path, transform=train_transform)\nval_dataset = ImageFolder(train_val_path, transform=val_transform)\n\nclass_names = train_dataset.classes\nprint(class_names) # list out all the classes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T09:17:47.111274Z","iopub.execute_input":"2025-03-15T09:17:47.111623Z","iopub.status.idle":"2025-03-15T09:17:55.272141Z","shell.execute_reply.started":"2025-03-15T09:17:47.11159Z","shell.execute_reply":"2025-03-15T09:17:55.27127Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Splitting the data into train and validation set\ndef split_train_val(tot_img, val_percentage=0.2, rnd=23):\n    # Here indices are randomly permuted \n    number_of_val = int(tot_img*val_percentage)\n    \n    np.random.seed(rnd)\n    indexs = np.random.permutation(tot_img)\n    return indexs[0:number_of_val], indexs[number_of_val:]\n\nrandomness = 1\nval_per = 0.2\n\nall_len = len(train_dataset)\n\nval_indices, train_indices = split_train_val(all_len, val_per, randomness)\n\nprint(val_indices, \"validation data:\", val_indices.shape)\nprint(train_indices, \"train data:\", train_indices.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T09:17:55.272945Z","iopub.execute_input":"2025-03-15T09:17:55.273286Z","iopub.status.idle":"2025-03-15T09:17:55.281793Z","shell.execute_reply.started":"2025-03-15T09:17:55.273253Z","shell.execute_reply":"2025-03-15T09:17:55.280881Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = Subset(train_dataset, train_indices)\nval_dataset = Subset(val_dataset, val_indices)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T09:17:55.282676Z","iopub.execute_input":"2025-03-15T09:17:55.282992Z","iopub.status.idle":"2025-03-15T09:17:55.295208Z","shell.execute_reply.started":"2025-03-15T09:17:55.282959Z","shell.execute_reply":"2025-03-15T09:17:55.294493Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img0, label0 = train_dataset[9627]\nprint(img0.shape, label0)\n\nimg1,label1 = val_dataset[20]\nprint(img1.shape, label1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T09:17:55.295939Z","iopub.execute_input":"2025-03-15T09:17:55.296228Z","iopub.status.idle":"2025-03-15T09:17:55.32437Z","shell.execute_reply.started":"2025-03-15T09:17:55.296205Z","shell.execute_reply":"2025-03-15T09:17:55.323543Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def show(img, label):\n    print(\"label-->\",class_names[label])\n    img = img.numpy().transpose((1, 2, 0)) # Channel first then height and width\n    # Если вы применили нормализацию, например, для ResNet18,\n    # то для визуализации и корректного отображения нужно сделать обратные преобразования\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    img = img * std + mean\n    img = np.clip(img, 0., 1.)\n    plt.imshow(img)\n\nshow(*train_dataset[6])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T09:17:55.327618Z","iopub.execute_input":"2025-03-15T09:17:55.327831Z","iopub.status.idle":"2025-03-15T09:17:55.705168Z","shell.execute_reply.started":"2025-03-15T09:17:55.327813Z","shell.execute_reply":"2025-03-15T09:17:55.703838Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Dataloaders","metadata":{}},{"cell_type":"code","source":"batch_size = 10\n\ntrain_dataloader = DataLoader(train_dataset, batch_size, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T09:17:55.707197Z","iopub.execute_input":"2025-03-15T09:17:55.70753Z","iopub.status.idle":"2025-03-15T09:17:55.711768Z","shell.execute_reply.started":"2025-03-15T09:17:55.707499Z","shell.execute_reply":"2025-03-15T09:17:55.710796Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Визуализация примеров","metadata":{}},{"cell_type":"code","source":"def show_images(images, labels, preds):\n    plt.figure(figsize=(10,10))\n    \n    images = images.cpu()\n    labels = labels.cpu()\n    preds = preds.cpu()\n    \n    for i, image in enumerate(images):\n        plt.subplot(1, batch_size, i + 1, xticks = [], yticks =[]) # x & y ticks are set to blank\n        image = image.numpy().transpose((1, 2, 0)) # Channel first then height and width\n        \n        # Если вы применили нормализацию, например, для ResNet18,\n        # то для визуализации и корректного отображения нужно сделать обратные преобразования\n        mean = np.array([0.485, 0.456, 0.406])\n        std = np.array([0.229, 0.224, 0.225])\n        image = image * std + mean\n        image = np.clip(image, 0., 1.)\n        plt.imshow(image)\n        \n        col = 'green' if preds[i] == labels[i] else 'red'\n        \n        plt.xlabel(f'{class_names[int(labels[i].numpy())]}')\n        plt.ylabel(f'{class_names[int(preds[i].numpy())]}', color=col)\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T09:17:55.712561Z","iopub.execute_input":"2025-03-15T09:17:55.712813Z","iopub.status.idle":"2025-03-15T09:17:55.726587Z","shell.execute_reply.started":"2025-03-15T09:17:55.712793Z","shell.execute_reply":"2025-03-15T09:17:55.725837Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"images, labels = next(iter(train_dataloader)) # Fetch the next batch of images\nshow_images(images, labels, labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T09:17:55.727323Z","iopub.execute_input":"2025-03-15T09:17:55.727576Z","iopub.status.idle":"2025-03-15T09:17:56.213907Z","shell.execute_reply.started":"2025-03-15T09:17:55.727555Z","shell.execute_reply":"2025-03-15T09:17:56.212837Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"images, labels = next(iter(val_dataloader))\nshow_images(images, labels, labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T09:17:56.214616Z","iopub.execute_input":"2025-03-15T09:17:56.214851Z","iopub.status.idle":"2025-03-15T09:17:56.693186Z","shell.execute_reply.started":"2025-03-15T09:17:56.214831Z","shell.execute_reply":"2025-03-15T09:17:56.692138Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Создание модели","metadata":{}},{"cell_type":"code","source":"'''\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n        # Convolutional layers\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n\n        # Fully connected layers\n        self.fc1 = nn.Linear(64 * 28 * 28, 256)\n        self.fc2 = nn.Linear(256, 4)  # Output size: 4 classes\n\n    def forward(self, x):\n        # current x.shape [batch_size, 3, 224, 224]\n        # First convolutional layer with ReLU and max pooling\n        x = F.relu(self.conv1(x))\n        # current x.shape [batch_size, 16, 224, 224]\n        x = F.max_pool2d(x, kernel_size=2, stride=2)  # Output size: (16, 112, 112)\n        # current x.shape [batch_size, 16, 112, 112]\n\n        # Second convolutional layer with ReLU and max pooling\n        x = F.relu(self.conv2(x))\n        # current x.shape [batch_size, 32, 112, 112]\n        x = F.max_pool2d(x, kernel_size=2, stride=2)  # Output size: (32, 56, 56)\n        # current x.shape [batch_size, 32, 56, 56]\n\n        # Third convolutional layer with ReLU and max pooling\n        x = F.relu(self.conv3(x))\n        # current x.shape [batch_size, 64, 56, 56]\n        x = F.max_pool2d(x, kernel_size=2, stride=2)  # Output size: (64, 28, 28)\n        # current x.shape [batch_size, 64, 28, 28]\n\n        # Flatten the tensor for the fully connected layer\n        x = x.view(x.size(0), -1)  # Flatten the output to (batch_size, 64*28*28)\n        # current x.shape [batch_size, 50176]\n\n        # First fully connected layer\n        x = F.relu(self.fc1(x))\n        # current x.shape [batch_size, 128]\n        \n        # Output layer\n        x = self.fc2(x)  # Output shape: (batch_size, 4)\n        # current x.shape [batch_size, 4]\n        return x\n        \n\nmodel = SimpleCNN()\nprint(model)\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T09:17:56.694152Z","iopub.execute_input":"2025-03-15T09:17:56.694507Z","iopub.status.idle":"2025-03-15T09:17:56.700746Z","shell.execute_reply.started":"2025-03-15T09:17:56.694463Z","shell.execute_reply":"2025-03-15T09:17:56.699808Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_resnet18 = torchvision.models.resnet18(pretrained=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T09:17:56.701743Z","iopub.execute_input":"2025-03-15T09:17:56.702068Z","iopub.status.idle":"2025-03-15T09:17:56.939555Z","shell.execute_reply.started":"2025-03-15T09:17:56.702015Z","shell.execute_reply":"2025-03-15T09:17:56.938799Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class_names","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T09:17:56.940412Z","iopub.execute_input":"2025-03-15T09:17:56.940701Z","iopub.status.idle":"2025-03-15T09:17:56.94673Z","shell.execute_reply.started":"2025-03-15T09:17:56.940671Z","shell.execute_reply":"2025-03-15T09:17:56.945987Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\nmodel = model.to(device)\n\ncriterion = torch.nn.CrossEntropyLoss()\n\noptimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T09:17:56.947508Z","iopub.execute_input":"2025-03-15T09:17:56.947764Z","iopub.status.idle":"2025-03-15T09:17:56.961188Z","shell.execute_reply.started":"2025-03-15T09:17:56.947731Z","shell.execute_reply":"2025-03-15T09:17:56.960276Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_resnet18.fc = torch.nn.Linear(in_features=512, out_features=4)\nmodel_resnet18 = model_resnet18.to(device)\n\ncriterion = torch.nn.CrossEntropyLoss()\n\noptimizer = torch.optim.Adagrad(model_resnet18.parameters(), lr=39e-5)\n#AdamW\n#6e-4 accuracy сильно скачет вверх-вниз\n#45e-5 accuracy сильно скачет вверх-вниз\n#35e-5 более стабильно, но все равно accuracy сильно скачет вверх-вниз (0,92 - 0,77 - 0,86)\n\n#Adagrad\n#35e-5 довольно быстро достигаем accuracy 0.9-0.92 и колеблемся в этих пределах, доползли до 0.94, в целом неплохо\n#31e-5 примерно так же, как в прошлый раз","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T09:17:56.961913Z","iopub.execute_input":"2025-03-15T09:17:56.962176Z","iopub.status.idle":"2025-03-15T09:17:57.112292Z","shell.execute_reply.started":"2025-03-15T09:17:56.962154Z","shell.execute_reply":"2025-03-15T09:17:57.111629Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def show_preds():\n    model_resnet18.eval()  # Setting the model to evaluation mode\n    images, labels = next(iter(val_dataloader))\n    \n    with torch.no_grad():\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        outputs = model_resnet18(images)\n        \n    _, preds = torch.max(outputs, 1)\n    show_images(images, labels, preds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T09:17:57.113009Z","iopub.execute_input":"2025-03-15T09:17:57.113246Z","iopub.status.idle":"2025-03-15T09:17:57.118591Z","shell.execute_reply.started":"2025-03-15T09:17:57.113227Z","shell.execute_reply":"2025-03-15T09:17:57.117554Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"show_preds()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T09:17:57.119291Z","iopub.execute_input":"2025-03-15T09:17:57.119535Z","iopub.status.idle":"2025-03-15T09:17:57.79908Z","shell.execute_reply.started":"2025-03-15T09:17:57.119515Z","shell.execute_reply":"2025-03-15T09:17:57.798127Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train(epochs):\n    print('Starting training..')\n    \n    train_losses = []\n    val_losses = []\n    val_accuracies = []\n    \n    for e in range(0, epochs):\n        print('='*20)\n        print(f'Starting epoch {e + 1}/{epochs}')\n        print('='*20)\n\n        train_loss = 0.\n        val_loss = 0.  # Not computing val_loss since we'll be evaluating the model multiple times within one epoch\n        \n        \n        model_resnet18.train() # set model to training phase\n        \n        for train_step, (images, labels) in enumerate(train_dataloader):\n            \n            images = images.to(device)\n            labels = labels.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model_resnet18(images)\n            loss = criterion(outputs, labels)\n            # Once we get the loss we need to take a gradient step\n            loss.backward() # Back propagation\n            optimizer.step() # Completes the gradient step by updating all the parameter values (we are using all parameters)\n            train_loss += loss.item() # Loss is a tensor which can't be added to train_loss so .item() converts it to float\n            \n            # Evaluating the model every 20th step\n            if train_step % 20 == 0:\n                print('Evaluating at step', train_step)\n\n                accuracy = 0\n\n                model_resnet18.eval() # set model to eval phase\n\n                all_preds = []\n                all_labels = []\n\n                for val_step, (images, labels) in enumerate(val_dataloader):\n                    \n                    images = images.to(device)\n                    labels = labels.to(device)\n                    \n                    with torch.no_grad():\n                        outputs = model_resnet18(images)\n                        \n                    loss = criterion(outputs, labels)\n                    val_loss += loss.item()\n\n                    _, preds = torch.max(outputs, 1)\n                    accuracy += sum((preds.cpu() == labels.cpu()).numpy()) # adding correct preds to acc\n\n                    all_preds.extend(preds.cpu().numpy())\n                    all_labels.extend(labels.cpu().numpy())\n\n\n                val_loss /= (val_step + 1)\n                accuracy = accuracy/len(val_dataset)\n                print(f'Validation Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}')\n\n                val_losses.append(val_loss)\n                val_accuracies.append(accuracy)\n\n                #show_preds()\n\n                model_resnet18.train()\n\n                if accuracy >= 0.95:\n                    print('Performance condition satisfied, stopping..')\n                    break\n\n        train_loss /= (train_step + 1)\n        train_losses.append(train_loss)\n\n        print(f'Training Loss: {train_loss:.4f}')\n    print('Training complete..')\n\n    plt.figure(figsize=(12, 5))\n    \n    # Plot training and validation loss\n    plt.subplot(1, 2, 1)\n    plt.plot(train_losses, label='Training Loss')\n    plt.plot(val_losses, label='Validation Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Training and Validation Loss')\n    plt.legend()\n    \n    # Plot validation accuracy\n    plt.subplot(1, 2, 2)\n    plt.plot(val_accuracies, label='Validation Accuracy', color='green')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.title('Validation Accuracy')\n    plt.legend()\n    \n    plt.show()\n\n    # Confusion Matrix\n    model_resnet18.eval()\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for images, labels in val_dataloader:\n            images = images.to(device)\n            labels = labels.to(device)\n            \n            outputs = model_resnet18(images)\n            _, preds = torch.max(outputs, 1)\n            \n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    cm = confusion_matrix(all_labels, all_preds)\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.title('Confusion Matrix')\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T09:17:57.800043Z","iopub.execute_input":"2025-03-15T09:17:57.80032Z","iopub.status.idle":"2025-03-15T09:17:57.812882Z","shell.execute_reply.started":"2025-03-15T09:17:57.800296Z","shell.execute_reply":"2025-03-15T09:17:57.811828Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Запуск обучения","metadata":{}},{"cell_type":"code","source":"%%time\n\ntrain(epochs=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T09:17:57.813825Z","iopub.execute_input":"2025-03-15T09:17:57.814156Z","iopub.status.idle":"2025-03-15T10:17:07.746547Z","shell.execute_reply.started":"2025-03-15T09:17:57.814122Z","shell.execute_reply":"2025-03-15T10:17:07.745587Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Создание класса TestDataset\nНаследуемся от базового класса Dataset, модифицируем метод get_item, который теперь возвращает только изображение, т.к. метка класса неизвестна и ее нужно предсказать в рамках соревнования","metadata":{}},{"cell_type":"code","source":"class TestDataset(Dataset):\n    def __init__(self, root_dir, transform=None): \n        self.root_dir = root_dir\n        self.transform = transform\n        self.filenames = sorted(os.listdir(root_dir))\n\n    def __len__(self):\n        return len(self.filenames)\n\n    def __getitem__(self, idx):\n        img_name = os.path.join(self.root_dir,\n                                self.filenames[idx])\n        \n        image = Image.open(img_name)\n        image = image.convert('RGB')\n        \n        if self.transform:\n            sample = self.transform(image)\n\n        return sample","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T10:17:07.747456Z","iopub.execute_input":"2025-03-15T10:17:07.747715Z","iopub.status.idle":"2025-03-15T10:17:07.752932Z","shell.execute_reply.started":"2025-03-15T10:17:07.747689Z","shell.execute_reply":"2025-03-15T10:17:07.75195Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"root_dir = '/kaggle/input/radiograph-classification-2025/Test'\ntest_dataset = TestDataset(root_dir, test_transform)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T10:17:07.755924Z","iopub.execute_input":"2025-03-15T10:17:07.756187Z","iopub.status.idle":"2025-03-15T10:17:07.772564Z","shell.execute_reply.started":"2025-03-15T10:17:07.756167Z","shell.execute_reply":"2025-03-15T10:17:07.771701Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img0 = test_dataset[112]\nprint(img0.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T10:17:07.773669Z","iopub.execute_input":"2025-03-15T10:17:07.773947Z","iopub.status.idle":"2025-03-15T10:17:07.791954Z","shell.execute_reply.started":"2025-03-15T10:17:07.77392Z","shell.execute_reply":"2025-03-15T10:17:07.791057Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_dataloader = DataLoader(test_dataset, batch_size, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T10:17:07.792607Z","iopub.execute_input":"2025-03-15T10:17:07.792844Z","iopub.status.idle":"2025-03-15T10:17:07.796774Z","shell.execute_reply.started":"2025-03-15T10:17:07.792824Z","shell.execute_reply":"2025-03-15T10:17:07.795791Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Predict на тестовой выборке","metadata":{}},{"cell_type":"code","source":"# Generate predictions\npredictions = []\n\nmodel_resnet18.eval()  # Set model to evaluation mode\n\n\nfor images in test_dataloader:\n    images = images.to(device)\n    \n    with torch.no_grad():\n        outputs = model_resnet18(images)\n        \n    _, preds = torch.max(outputs, 1)\n    \n    preds = preds.cpu()\n    predictions.extend(preds.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T10:17:07.797616Z","iopub.execute_input":"2025-03-15T10:17:07.797837Z","iopub.status.idle":"2025-03-15T10:17:08.915245Z","shell.execute_reply.started":"2025-03-15T10:17:07.797817Z","shell.execute_reply":"2025-03-15T10:17:08.914479Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Формирование файла submission","metadata":{}},{"cell_type":"code","source":"predictions_df = pd.DataFrame({'ImageId': range(1, len(predictions) + 1), 'Label': predictions})\n\npredictions_df","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-03-15T10:17:08.916079Z","iopub.execute_input":"2025-03-15T10:17:08.916371Z","iopub.status.idle":"2025-03-15T10:17:08.928041Z","shell.execute_reply.started":"2025-03-15T10:17:08.916342Z","shell.execute_reply":"2025-03-15T10:17:08.9272Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions_df.to_csv('submission.csv', index = False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T10:17:08.928901Z","iopub.execute_input":"2025-03-15T10:17:08.929275Z","iopub.status.idle":"2025-03-15T10:17:08.943221Z","shell.execute_reply.started":"2025-03-15T10:17:08.929231Z","shell.execute_reply":"2025-03-15T10:17:08.942429Z"}},"outputs":[],"execution_count":null}]}